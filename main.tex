% !TEX encoding = UTF-8 Unicode
\documentclass[aodsor,preprint]{imsart}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage[authoryear,round]{natbib}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
%\usepackage{ngerman}


% settings
%\pubyear{2005}
%\volume{0}
%\issue{0}
%\firstpage{1}
%\lastpage{8}
%\arxiv{arXiv:0000.0000}


\numberwithin{equation}{section}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{corollary}[thm]{Corollary}
\newtheorem{remark}[thm]{Remark}
\newtheorem*{remark*}{Remark}
\newtheorem{assumption}[thm]{Assumption}

% customize math operators
\newcommand{\E}{{\mathbb E}}
\newcommand{\Var}{{\mathbb{V}ar}}
\newcommand{\ind}{\mathbbm{1}} % indicator function

\begin{document}

\begin{frontmatter}
\title{The Bootstrap for Causal Inference with an application to marriage market incentives}
\runtitle{The Bootstrap for Causal Inference}

\begin{aug}
\author{\fnms{Gregor} \snm{Steiner}\ead[label=e1]{first@somewhere.com}}



\runauthor{Gregor Steiner}

\affiliation{University of Vienna}

\end{aug}

\begin{abstract}
The causal bootstrap is a modification of the classical bootstrap that accounts for uncertainty in the assignment of treatment. This allows for inference on causal parameters. This paper outlines the procedure for two-sided inference on the Average Treatment Effect and applies it to the problem of marriage market incentives. I find evidence that women signal less professional ambition when their preferences are observed by potential romantic partners.
\end{abstract}

\begin{keyword}[class=MSC]
\kwd{62D05}
\kwd{62D20}
\kwd{62G09}
\end{keyword}

\begin{keyword}
\kwd{causal inference}
\kwd{bootstrap}
\end{keyword}

\end{frontmatter}

\section{Introduction}

The bootstrap was originally proposed by \cite{Efron_1982} and is frequently used in situations where standard parametric inference is difficult or even impossible. \cite{Imbens_2021} suggest a modified version of the bootstrap for causal inference problems.

Causal inference from a statistical perspective is concerned with measuring the effects of causes \citep{Holland_1986}. In this specific case, we are concerned with situations where units from a population either receive some treatment or they do not. The treatment is the cause and the aim is to measure the effect of said cause. For example, the treatment might be a drug whose effect on certain health parameters we want to measure. 

In such situations we face two types of uncertainty: sampling and design uncertainty. Sampling uncertainty stems from the fact that we only observe a (usually small) sample of the population, while design uncertainty refers to the stochastic nature of treatment assignment. A short example will help to clarify this difference: Assume a pharmaceutical company wants to test their drug in a randomized trial. They randomly select patients to participate in this experiment. Among the chosen patients they randomly assign treatment, that is half of them are treated at random and the other half acts as the control group. Now there are two sources of uncertainty: They could have chosen different patients to participate (sampling uncertainty) and they could have assigned treatment differently among the chosen patients (design uncertainty). While classical statistics focuses on sampling uncertainty, the second choice (assigning treatment) is somewhat unique to causal inference settings. Therefore, the bootstrap needs to be modified for such settings.

In this paper, constructing confidence intervals for the Average Treatment Effect (ATE) based on \cite{Imbens_2021} is extensively discussed. The ATE is a measure of the average effect of some treatment on some outcome of interest. As an illustrative example, I then apply the method to marriage market incentives: Men tend to favor less ambitious romantic partners. This creates a trade-off for women: Actions that lead to professional success may lead to failure in the marriage market. Utilizing experimental data by \cite{Bursztyn_2017}, I test whether women are more cautious about signaling professional ambition when potential partners observe their preferences. In particular, I estimate the ATE and bootstrap confidence intervals for two measures of professional ambition: desired compensation and working hours.

Section \ref{setting} introduces the setting and some notation, Section \ref{bootstrap} explains the causal bootstrap, Section \ref{application} applies it to the marriage market incentives problem, and Section \ref{Conclusion} concludes.



\section{Setting} \label{setting}

This paper focuses on situations of the following form: A sample of $n$ units is either exposed to a treatment (treatment group) or not (control group), and we are interested in the causal effect of said treatment for the entire population of $N \geq n$ units. More specifically, we observe an outcome $Y_i$ and a binary variable $D_i \in \{0, 1\}$ (the treatment) for $i = 1,\ldots, n$. $D_i$ indicates treatment for unit $i$ and takes on the value $1$ if unit $i$ is treated, and $0$ else.

To formalize the concept of causality, we use Rubin's potential outcome framework \citep{Rubin_2005}. Note that there are other approaches to causality, for example the graphical methods by Judea Pearl \citep[see for example][]{Pearl_2009}, but Rubin's framework is best suited for our purposes. We denote the outcome as $Y(1)$ if treated and as $Y(0)$ otherwise. The actual outcome $Y_i$ is simply
$$
Y_i = D_i Y_i(1) + (1-D_i) Y_i(0) = \begin{cases}
	Y_i(1), & D_i = 1 \\
	Y_i(0), & D_i = 0.
\end{cases}
$$
It is assumed, that the potential outcomes $\big(Y_i(0), Y_i(1)\big)$ for unit $i$ are independent of the treatment status of other units. This is commonly called the Stable Unit Treatment Value Assumption (SUTVA).

For some unit $i$ the unit-causal effect is $Y_i(1) - Y_i(0)$, but only one of these is actually observed. Therefore, the unit-causal effect cannot be identified. \cite{Holland_1986} calls this the fundamental problem of causal inference.

However, we can identify the Average Treatment Effect (ATE), which is the average difference between outcomes in the treated and untreated state,
$$
\tau := ATE = \frac{1}{N} \sum_{i = 1}^{N} \big( Y_i(1) - Y_i(0) \big).
$$
In some sense the ATE is just the expected change in the outcome caused by treatment. In other words, this is the change we would expect if an untreated unit becomes treated. Let $n_0, n_1$ denote the number of untreated and treated units respectively in a sample of $n = n_0 + n_1$ observations and let $R_i \in \{0, 1\}$ be an indicator whether the i-th population unit is included in the sample. Then the standard difference in means estimator for $\tau$ is
$$
\widehat{\tau} = \frac{1}{n_1} \sum_{i = 1}^{N} R_i D_i Y_i - \frac{1}{n_0} \sum_{i = 1}^{N} R_i (1 - D_i) Y_i = \bar{Y}_1 - \bar{Y}_0.
$$

Let
$$S_D^2 = \frac{1}{N-1} \sum_{i = 1}^{N} \big(Y_i(D) - \bar{Y}(D) \big)^2,$$
where $\bar{Y}(D) = \frac{1}{N} \sum_{i = 1}^{N} Y_i(D)$ for $D\in \{0, 1\}$, and
$$S_{01}^2 = \frac{1}{N-1} \sum_{i = 1}^{N} \big( Y_i(1) - Y_i(0) - \tau \big)^2.$$ Then the true variance of $\widehat{\tau}$ is
$$
\Var(\widehat{\tau}) = \frac{S_0^2}{n_0} + \frac{S_1^2}{n_1} - \frac{S_{01}^2}{N}.
$$
Traditional estimators ignored the $S_{01}^2$ term, which leads to overestimation of the true variance. \cite{aronow_2014} proposed an estimator based on the lower bound:
$$
\widehat{\Var}_{AGL} = \frac{\widehat{S}_0^2}{n_0} + \frac{\widehat{S}_1^2}{n_1} - \frac{\widehat{S}_{01}^2}{N},
$$
where $\widehat{S}_1^2 = \frac{1}{n_1 - 1} \sum_{i = 1}^{N} R_i D_i Y_i$, $\widehat{S}_0^2 = \frac{1}{n_0 - 1} \sum_{i = 1}^{N} R_i (1 - D_i) Y_i$, and $\widehat{S}_{01}^2$ estimates the lower bound of $S_{01}^2$ \citep[see][Section 3]{aronow_2014}. Therefore, $\widehat{\Var}_{AGL}$ estimates the upper bound of $\Var(\widehat{\tau})$.

\section{The causal bootstrap} \label{bootstrap}

This section introduces the causal bootstrap proposed by \cite{Imbens_2021} and closely follows their Sections 2, 3, and 5.

\subsection{Preliminaries and assumptions}
The aim of the causal bootstrap is to account for the stochastic nature of the treatment assignment. This takes us back to the potential outcome problem. For each observation we only observe one potential outcome. A natural approach is to impute the missing potential outcome. To do this, it is useful to start with the population joint distribution function of the pair of potential outcomes,
$$
F_{01}^p (y_0, y_1) := \frac{1}{N} \sum_{i = 1}^{N} \ind\{Y_i(0) \leq y_0, Y_i(1) \leq y_1\}.
$$
Unfortunately, there is no consistent estimator for $F_{01}^p$, but we can express it as a function of the marginal distribution functions,
$$
F_{01}^p (y_0, y_1) = C(F_0^p(y_0), F_1^p(y_1)),
$$
where $C: [0, 1]^2 \to [0, 1]$ is non-decreasing in both arguments. Such a copula $C$ is guaranteed to exist by Sklar's theorem \citep[see for example][]{Nelsen_2007}, but it need not be unique. Using $C$ we can obtain an estimate $\widehat{F}_{01} := C(\widehat{F}_0, \widehat{F}_1)$. However, since $C$ need not be unique, we want to choose it conservatively. If the first four moments of $F_0(y_0)$ and $F_1(y_1)$ are bounded, the isotone coupling,
$$
C^{iso}(u, v) := \min(u, v),
$$
is the least favorable coupling in the sense that it attains an upper bound on the asymptotic variance of $\widehat{\tau}$ \citep[][Sections 2.4 and 2.5]{Imbens_2021}. The isotone coupling allows us to impute the missing potential outcomes as
\begin{align} \label{imputation1}
	Y_i(0) &= \begin{cases}
		Y_i, & D_i = 0 \\
		\widehat{F}_0^{-1}(\widehat{F}_1(Y_i)), & D_i = 1
	\end{cases} \\
	\label{imputation2}
	Y_i(1) &= \begin{cases}
		\widehat{F}_1^{-1}(\widehat{F}_0(Y_i)), & D_i = 0 \\
		Y_i, & D_i = 1.
	\end{cases}
\end{align}
However, this is only valid for two-sided inference on the ATE. For other problems the appropriate least favorite coupling will likely be a different one.

Furthermore, we need a few assumptions:

\begin{assumption} \label{as1}
	The observed units are sampled at random from the population, that is ($Y_i(0), Y_i(1)$) are independent of $R_i$.
\end{assumption}
\begin{assumption} \label{as2}
	Treatment assignment is randomized, that is ($Y_i(0), Y_i(1)$) are independent of $D_i$\footnote{The approach can be extended to observational studies, if the outcome is independent of treatment assignment conditional on observable covariates (unconfoundedness). For details see \cite{Imbens_2021} Section 6.}.
\end{assumption}
\begin{assumption} \label{as3}
	The outcome $Y_i(D_i)$ for unit $i$ is not affected by other units' treatment status, that is the Stable Unit Treatment Value Assumption (SUTVA) holds.
\end{assumption}

After establishing these preliminaries, we can finally outline the bootstrap procedure below.

\subsection{The bootstrap procedure}

Given a sample generated in accordance with the assumptions above, we are interested in the distribution of the t-ratio
$$
T = \frac{\widehat{\tau} - \tau}{\widehat{\sigma} / \sqrt{n}},
$$
where $\widehat{\tau}$ is the estimator of the ATE and $\widehat{\sigma} = \widehat{\sigma}(\widehat{F}_0, \widehat{F}_1)$ is the upper bound standard deviation.

The proposed algorithm consists of four main steps:

\begin{enumerate}
	
	\item \textit{Generating the population:} To generate an artificial population of size $N$, the $n$ sampled observations are replicated according to the following procedure:
	\begin{enumerate}
		\item Split the sample in treated and untreated, i.e. $(Y_j^0)_{j=1}^{n_0}$ and $(Y_j^1)_{j=1}^{n_1}$ and order them in an increasing fashion.
		\item Let $N_0 = \lceil \frac{n_0}{n} N \rceil$, $N_1 = N - N_0$ and include 
		$$M_j^0 := \lceil \frac{j}{n_0} N_0 \rceil - \lceil \frac{j-1}{n_0} N_0 \rceil$$
		copies of $Y_j^0$ with $D_j = 0$ for $j = 1, \ldots, n_0$, and 
		$$M_j^1 := \lceil \frac{j}{n_1} N_1 \rceil - \lceil \frac{j-1}{n_1} N_1 \rceil$$
		copies of $Y_j^1$ with $D_j = 1$ for $j = 1, \ldots, n_1$.
	\end{enumerate}
	Thus, we end up with a population $(Y_j, D_j)_{j=1}^N$, where the pairs of values are drawn from the sample according to the rule above. 
	
	\item \textit{Imputing missing potential values:} The next step is imputing the missing potential outcomes. That is, for treated units we want an estimate of $Y_i(0)$ and for untreated units of $Y_i(1)$. As already discussed above, we simply use \ref{imputation1} and \ref{imputation2}, where $\widehat{F}_0$ and $\widehat{F}_1$ are the empirical distribution functions estimated from the untreated and treated units respectively.
	
	\item \textit{Resampling}: For each bootstrap replication $b = 1,\ldots, B$, $n$ pairs of ($Y_{ib}^*(0), Y_{ib}^*(1)$) are randomly drawn from the generated population. In the drawn bootstrap sample, treatment is randomly assigned with probability $p:= \mathbb{P}(D_i = 1 | R_i = 1)$, that is the probability of treatment in the actual sample. Then, we can compute the bootstrap sample estimators $\widehat{\tau}_b^*$ and $\widehat{\sigma}_b^*$, and record the t-ratio
	$$
	T_b^* = \frac{\widehat{\tau}_b^* - \widehat{\tau}}{\widehat{\sigma}_b^* / \sqrt{n}}.
	$$
	
	\item \textit{Confidence intervals:} Let $\widehat{G}(t) = \frac{1}{B} \sum_{i=1}^{B} \ind\{T_b^* \leq t\}$ be the empirical distribution function of the t-ratios obtained through resampling. Confidence intervals can then be constructed as 
	\begin{align} \label{CI}
		CI(1-\alpha) = \left[ \widehat{\tau} - \frac{\widehat{\sigma}}{\sqrt{n}} \widehat{G}^{-1}(1-\alpha); \widehat{\tau} - \frac{\widehat{\sigma}}{\sqrt{n}} \widehat{G}^{-1}(\alpha)  \right],
	\end{align}
	where $\widehat{G}^{-1}$ denotes the quantile function of the empirical distribution $\widehat{G}$.
	
\end{enumerate}



\subsection{Asymptotics}

This section provides a characterization of the asymptotic behavior of the bootstrap procedure.

\begin{thm}
	Assume assumptions \ref{as1}, \ref{as2}, and \ref{as3} hold and the first four moments of $F_0(y_0)$ and $F_1(y_1)$ are bounded, then $\widehat{\tau}$ and $\widehat{\sigma}$ are consistent for $\tau$ and $\sigma$.
\end{thm}

\begin{proof}
	By Glivenko-Cantelli \citep[see][page 265]{Asymptotic}, $\widehat{F}_0(y_0)$ and $\widehat{F}_1(y_1)$ converge almost surely to the true distribution functions, $F_0(y_0)$ and $F_1(y_1)$. This implies convergence in probability and therefore consistency.
	
	Recall that $\tau$ and $\sigma$ are functions of the marginal distribution functions, so they can be written as $\tau(F_0(y_0), F_1(y_1))$ and $\sigma(F_0(y_0), F_1(y_1))$. Under the assumption that the first four moments of $F_0(y_0)$ and $F_1(y_1)$ are bounded, they are  even continous functions of $F_0(y_0)$ and $F_1(y_1)$ \citep[see][section 5.1]{Imbens_2021}. Thus, by the continous mapping theorem, the estimators converge in probability to the true parameters (or parameter bounds),
	$$
	\widehat{\tau}\big(\widehat{F}_0(y_0), \widehat{F}_1(y_1)\big) \overset{p}{\to} \tau\big(F_0(y_0), F_1(y_1)\big)
	$$
	and
	$$
	\widehat{\sigma}\big(\widehat{F}_0(y_0), \widehat{F}_1(y_1)\big) \overset{p}{\to} \sigma\big(F_0(y_0), F_1(y_1)\big),
	$$
	that is they are consistent.
\end{proof}

\begin{thm}
	Assume assumptions \ref{as1}, \ref{as2}, and \ref{as3} hold, then 
	$$
	\frac{\widehat{\tau} - \tau}{\widehat{\sigma} / \sqrt{n}} \overset{d}{\to} \mathcal{N} \left( 0, \frac{\sigma^2(F_{01}^p)}{\sigma^2(F_0^p, F_1^p)}\right),
	$$
	where $\sigma^2(F_{01}) = \lim_{n \to \infty} n \Var_{F_{01}} (\widehat{\tau})$ and $\overset{d}{\to}$ denotes convergence in distribution.
\end{thm}

The proof of this result is rather involved, the interested reader is referred to \cite{Imbens_2021}, Appendix C.


The next step is establishing a central limit theorem (CLT) for the bootstrap analogues. Let $\widehat{F}_0^*$ and $\widehat{F}_1^*$ denote the empirical distribution functions for a bootstrap replication for the untreated and treated observations respectively. Furthermore, let $\widehat{\tau}^* = \tau(\widehat{F}_0^*, \widehat{F}_1^*)$ and $\widehat{\sigma}^* = \sigma(\widehat{F}_0^*, \widehat{F}_1^*)$ denote the estimators' bootstrap analogues.
		
\begin{thm} \label{Thm3.2}
	Assume assumptions \ref{as1}, \ref{as2}, and \ref{as3} hold, then
	$$
	\frac{\widehat{\tau}^* - \widehat{\tau}}{\widehat{\sigma}^* / \sqrt{n}} \overset{d}{\to} \mathcal{N} \left( 0, 1 \right),
	$$
	that is the bootstrapped t-ratio converges in distribution to a standard normal.
\end{thm}

This result can be proven in a similar manner as \ref{Thm3.2} \citep[see][Section 5 and Appendix C]{Imbens_2021}.

Finally, we can use these results to show that the confidence intervals obtained by the bootstrap procedure are indeed valid, that is the asymptotic coverage probability is at least as large as the nominal coverage probability.

\begin{thm}
	Assume assumptions \ref{as1}, \ref{as2}, and \ref{as3} hold, then the confidence interval obtained by \ref{CI} is asymptotically valid, that is,
	$$
	\lim_{n \to \infty} \inf_{F_{01}^p} \mathbb{P}_{F_{01}^p} \big(\tau(F_{01}^p) \in CI(1-\alpha) \big) \geq 1 - \alpha.
	$$
\end{thm}

This result follows from the previous results and the definition of the variance bound \citep[see][Corollary 5.1]{Imbens_2021}. Thus, the bootstrap procedure is indeed asymptotically valid.



\section{Application to marriage market incentives} \label{application}

In this section I apply the method outlined above to experimental data by \cite{Bursztyn_2017}. Experimental data is convenient since the randomization assumption can be guaranteed to hold. This is rarely the case for observational data. 

Men tend to favor romantic partners who are less ambitious than themselves. This creates a trade-off for women in the labor market: Actions that lead to professional success may lead to misfortune in the marriage market. Thus, single women may be more cautious about signaling professional ambition than their married counterparts. \cite{Bursztyn_2017} test this hypothesis in an experiment.

\subsection{Experimental design}
They study students in an elite MBA program. Their primary experiment took place during a career center on the first day of the MBA program, where students were asked to fill out a questionnaire that would be used to place them in a summer internship. Summer internships are incredibly important for MBA students: A substantial share of students ends up working for the firm where they did their summer internship. Therefore, this should be a high stakes event for students. Their answers influence what jobs the career center views as a good match for them.

Students randomly received one of two versions of the questionnaire: A public or a private one. The public version said that 'your' answers would be discussed in class. The private version, on the other hand, told the students that anonymized versions would be discussed in class. Students that received the public version are thus led to believe that their answers might be observable by their classmates. However, the students did not know that there were two versions of the questionnaire or that they were part of a research project.

The heart of the questionnaire is a series of questions about their desired compensation, preferred hours of work per week, and willingness to travel for work. The women that received the public version (treatment) face a trade-off: Providing answers that present them favorably to the labor market may make them less attractive to their potential romantic partners. Thus, one would expect that the answers by treated single-women differ from those by their married or male classmates. At the same time, this effect should be less-pronounced or non-existent among the untreated.


\subsection{Data}

\cite{Bursztyn_2017} make their data available for replication purposes. Table \ref{SumStats} shows summary statistics for the variables of interest. In total, the sample includes 354 students, 346 of which provided information on their sex and marital status. Out of those 346, 60 students are female and single.

We refer to the public version of the questionnaire as treatment (i.e. $D_i = 1$ if student $i$ received the public version), while the students who received the private version act as the control group. Out of the 60 single women, 29 received the treatment.

\begin{table}[!htbp] \centering \renewcommand*{\arraystretch}{1.1}\caption{Summary Statistics}\label{SumStats}\resizebox{\textwidth}{!}{
		\begin{tabular}{lrrrrrrr}
			\hline
			Variable & N & Mean & Std. Dev. & Min & Pctl. 25 & Pctl. 75 & Max \\ 
			\hline
			treatment & 354 &  &  &  &  &  &  \\ 
			... Private & 176 & 49.7\% &  &  &  &  &  \\ 
			... Public & 178 & 50.3\% &  &  &  &  &  \\ 
			male & 354 & 0.681 & 0.467 & 0 & 0 & 1 & 1 \\ 
			single & 346 & 0.474 & 0.5 & 0 & 0 & 1 & 1 \\ 
			class & 346 &  &  &  &  &  &  \\ 
			... Female \& Non-single & 51 & 14.7\% &  &  &  &  &  \\ 
			... Female \& Single & 60 & 17.3\% &  &  &  &  &  \\ 
			... Male \& Non-single & 131 & 37.9\% &  &  &  &  &  \\ 
			... Male \& Single & 104 & 30.1\% &  &  &  &  &  \\ 
			compensation & 353 & 136.494 & 33.856 & 75 & 112.5 & 162.5 & 250 \\ 
			hours & 353 & 51.66 & 8.779 & 40 & 45.5 & 55.5 & 80\\ 
			\hline
		\end{tabular}
	}
\end{table}

To draw conclusions about the effect of observability on professional ambition, we focus on desired compensation and hours of work per week. The respective questions on the questionnaire are \textit{What is your desired level of compensation?} and \textit{How many hours per week are you willing to work on a regular basis?} Many students provided their answers as a range, e.g. 175-200 (in thousand USD) or 50-55 hours. Such answers are transformed by taking the mean of the range.

Table \ref{mean values} shows the mean values by group and treatment. To be consistent with the hypothesis that women display less professional ambition when potential romantic partners can observe their preferences, the difference in means should be greater among single females. This seems to be the case for desired compensation. For willingness to work long hours, however, the difference between single and non-single women is rather small. 

\begin{table}[!h]
	\centering
	\caption{Mean values for desired compensation and hours by type and treatment}
	\label{mean values}
	\begin{tabular}{l|r|r|r|r|r|r}
		\hline
		& \multicolumn{3}{c|}{Compensation} & \multicolumn{3}{|c}{Hours} \\
		\hline
		& Private & Public & Difference & Private & Public & Difference \\
		\hline
		Female \& Non-single & 134.72 & 132.29 & -2.43 & 52.33 & 48.60 & -3.73\\
		Female \& Single & 130.65 & 113.79 & -16.85 & 51.88 & 48.33 & -3.54\\
		Male \& Non-single & 140.67 & 133.73 & -6.94 & 51.06 & 53.88 & 2.82\\
		Male \& Single & 144.69 & 145.31 & 0.62 & 51.80 & 52.16 & 0.36\\
		\hline
	\end{tabular}
\end{table}


\subsection{Bootstrapping confidence intervals}

The next step is using the bootstrap procedure to construct confidence intervals for the ATE based on the difference in means estimator. In particular, I want to focus on desired compensation and hours of work among single women, since a significant effect for this group would confirm the hypothesis discussed by \cite{Bursztyn_2017}. To the best of my knowledge, no R implementation for the causal bootstrap by \cite{Imbens_2021} exists. That is why I write my own function for the bootstrap procedure. The code is available in the Appendix \ref{Code}. Alternatively, the code is also available in a more structured form on my \href{https://github.com/gregorsteiner/CausalBootstrap}{Github}.

It is important to consider what population we generalize to, that is what $N$ is in this case. It seems reasonable to take all single females in the US who are close in age to the students studied by \cite{Bursztyn_2017}. According to the 2010 census, the female population in the age range 20-29 is about 21 million\footnote{Population size by sex for selected age groups is available at the \href{https://www.census.gov/data/tables/time-series/demo/popest/2010s-national-detail.html}{Census website}.}. Unfortunately, information on marital status is not available. Using the proportion of singles in our sample, yields about 9.95 million single women in the population. Population sizes for the other three classes can be obtained analogously.

Table \ref{CIs} shows the estimated 95\% confidence intervals for all four classes. For desired compensation among single females, the upper bound is well below zero. This indicates a significant average effect of the treatment\footnote{I refer to the effect as significant if the 95\% confidence interval does not include zero. This means that an inverted test used to obtain the confidence interval would reject the null hypothesis of a zero effect at the 5\% level.}. On average, the students demand between USD 2,600 and USD 26,000 less in compensation if they received the public version of the questionnaire. This makes the magnitude of the effect economically very relevant. However, for the desired number of weekly work hours, the confidence interval includes zero. There seems to be no significant effect for the desired hours of weekly work. Surprisingly, the upper bound of the non-single females is below zero, indicating a significant ATE for that group. In the male classes, there are no significant effects.

\begin{table}[!h]
	\centering
	\caption{95\% Confidence Intervals for the ATE}
	\label{CIs}
	\begin{tabular}{l|r|r|r|r}
		\hline
		& \multicolumn{2}{c|}{Compensation} & \multicolumn{2}{|c}{Hours} \\
		\hline
		& Lower Bound & Upper Bound & Lower Bound & Upper Bound\\
		\hline
		Female \& Non-single & -12.97 & 12.45 & -5.28 & -0.28\\
		\hline
		Female \& Single & -25.77 & -2.63 & -4.28 & 2.72\\
		\hline
		Male \& Non-single & -16.59 & 4.03 & -4.24 & 1.47\\
		\hline
		Male \& Single & -11.95 & 10.28 & -4.22 & 1.62\\
		\hline
	\end{tabular}
\end{table}

Thus, I can only partially confirm the results obtained by \cite{Bursztyn_2017}. They also obtain a significant effect for hours of work. However, they use standard errors from regressing hours on treatment. This is conceptually different from what is done here. While the causal bootstrap also accounts for design uncertainty, their approach only accounts for sampling uncertainty.

However, some doubts remain. First of all, the sample for the group of interest (female \& single) contains only 60 observations. This is a rather small sample. Also, it is doubtful whether elite MBA students are really representative of the general population. In many dimensions they likely are not. At the very least, they are not a random sample from the population, which violates the random sampling assumption. Thus, these results should be taken with a grain of salt. 



\section{Conclusion} \label{Conclusion}

The causal bootstrap by \cite{Imbens_2021} is a powerful alternative to the classical bootstrap for situations where one is interested in inference on causal parameters. It provides asymptotically valid confidence intervals under randomized treatment assignment. This paper focuses on two-sided inference for the ATE and presents the bootstrap procedure for such settings.

As an illustrative example, the procedure is then applied to experimental data by \cite{Bursztyn_2017}. I find some evidence for the hypothesis that single women display less professional ambition when their preferences are observed by their peers. In particular, they decrease their desired compensation significantly when they think their answers may be observable. Unlike \cite{Bursztyn_2017} I find no significant effect for desired hours of weekly work. 


\section*{Acknowledgements}

I would like to thank an anonymous referee and Prof. Jirak for helpful comments that substantially improved this paper.



%========= Appendix ==========

\newpage

\appendix
\section{Code} \label{Code}

\begin{lstlisting}[language=R, basicstyle=\tiny\tiny]
	######### Functions ######### 
		
	# function provided by Aronow, Green & Lee (2014)
	sharp.var <- function(yt, yc, N=length(c(yt,yc)), upper=TRUE) {
		m <- length(yt)
		n <- m + length(yc)
		FPvar <- function(x,N) (N-1)/(N*(length(x)-1)) * sum((x - mean(x))^2)
		yt <- sort(yt)
		if(upper == TRUE) yc <- sort(yc) else
		yc <- sort(yc,decreasing=TRUE)
		p_i <- unique(sort(c(seq(0,n-m,1)/(n-m),seq(0,m,1)/m))) -
		.Machine$double.eps^.5
		p_i[1] <- .Machine$double.eps^.5
		yti <- yt[ceiling(p_i*m)]
		yci <- yc[ceiling(p_i*(n-m))]
		p_i_minus <- c(NA,p_i[1: (length(p_i)-1)])
		return(((N-m)/m * FPvar(yt,N) + (N-(n-m))/(n-m) * FPvar(yc,N)
		+ 2*sum(((p_i-p_i_minus)*yti*yci)[2:length(p_i)])
		- 2*mean(yt)*mean(yc))/(N-1))
	}
	
	
	
	# function for the AGL variance
	Var_AGL <- function(Y, D, n0, n1, N){
		# get S1^2 and S0^2
		S1.2 <- var(Y[D == 1])
		S0.2 <- var(Y[D == 0])
		
		# S01^2
		S01.2 <- S0.2 + S1.2 - 2 * sharp.var(Y[D == 1], Y[D == 0], upper = TRUE)
		
		# in total
		Var <- S0.2 / n0 + S1.2 / n1 - S01.2 / N
		return(Var)
		
	}
	
	
	# bootstrap function
	causal_boot <- function(data, dep.var, treatment, N, B = 100, alpha = 0.05){
		
		# define treatment and outcome
		D <- data[, treatment]
		Y <- data[, dep.var]
		
		# remove NAs
		na.ind <- !is.na(Y) & !is.na(D)
		Y <- Y[na.ind]
		D <- D[na.ind]
		
		# some parameters
		p <- mean(D) # proportion of treated observations
		n <- length(Y) # observations in the sample
		n1 <- sum(D)
		n0 <- n - n1
		N0 <- ceiling((n0 / n) * N)
		N1 <- N - N0
		
		
		# estimators
		tau.hat <- mean(Y[D == 1], na.rm = TRUE) - mean(Y[D == 0], na.rm = TRUE)
		sigma.hat <- sqrt(Var_AGL(Y, D, n0, n1, N))
		
		# generate empirical population
		dat.boot <- do.call(rbind, Map(function(d, n.int, N.int){
			# select treatment
			dat <- data.frame(Y = Y[D == d], D = D[D == d])
			
			# order in an increasing fashion
			dat <- dat[order(dat$Y), ]
			
			# define rank
			#dat$U <- 1:nrow(dat) / n.int
			
			# loop over rows and include copies of them
			for (i in 1:n.int) {
				# compute number of inclusions
				M <- ceiling(i / n.int * N.int) - ceiling((i - 1) / n.int * N.int)
				
				# assign
				if(i == 1) store <- cbind(Y = rep(dat[i, "Y"], M),
				D = rep(dat[i, "D"], M))
				
				else{
					# try to generate replications, if an error occurs, we simply skip this one
					append <- cbind(Y = rep(dat[i, "Y"], M), D = rep(dat[i, "D"], M))
					
					# if the append object is a numeric matrix, we add it to the store object
					if(is.matrix(append) & is.numeric(append)){
						store <- rbind(store, append)
					} 
				}
				
			}
			
			# return
			return(store)
			
		}, c(0, 1), list(n0, n1), list(N0, N1)))
		
		
		# get empirical cdfs (F_1 and F_0)
		F1 <- ecdf(dat.boot[dat.boot[, "D"] == 1, "Y"])
		F0 <- ecdf(dat.boot[dat.boot[, "D"] == 0, "Y"])
		
		
		# impute missing potential outcomes
		dat.boot <- cbind(dat.boot,
		"Y0" = ifelse(dat.boot[, "D"] == 0,
		dat.boot[, "Y"],
		quantile(F0, F1(dat.boot[, "Y"]))), 
		"Y1" = ifelse(dat.boot[, "D"] == 1,
		dat.boot[, "Y"],
		quantile(F1, F0(dat.boot[, "Y"]))))
		
		T.store <- numeric(B)
		
		for (i in 1:B) {
			# randomly draw from population
			dat.boot.it <- dat.boot[sample(1:nrow(dat.boot), size = n), ]
			
			
			# randomly assign treatment (TRUE is treatment)
			Treat <- sample(c(FALSE, TRUE), size = n,
			replace = TRUE, prob = c(1 - p, p))
			# add to df
			dat.boot.it <- cbind(dat.boot.it,
			"Treat.Boot" = as.numeric(Treat),
			"Y.Boot" = ifelse(Treat,
			dat.boot.it[, "Y1"],
			dat.boot.it[, "Y0"]))
			
			# sample sizes
			n1.boot <- sum(Treat)
			n0.boot <- n - n1.boot
			
			# compute bootstrap sample statistics
			tau.hat.boot <- mean(dat.boot.it[Treat, "Y.Boot"]) - 
			mean(dat.boot.it[!Treat, "Y.Boot"])
			sigma.hat.boot <- sqrt(Var_AGL(Y = dat.boot.it[, "Y.Boot"],
			D = dat.boot.it[, "Treat.Boot"],
			n0 = n0.boot, n1 = n1.boot, N = N))
			
			
			# compute t-ratio
			T.store[i] <- (tau.hat.boot - tau.hat) / (sigma.hat.boot / sqrt(n))
			
		}
		
		# get ecdf of t-ratios
		G <- ecdf(T.store)
		
		# compute and return CI
		CI <- c("Lower" = tau.hat - quantile(G, 1 - alpha, names = FALSE) * sigma.hat / sqrt(n),
		"Upper" = tau.hat - quantile(G, alpha, names = FALSE) * sigma.hat / sqrt(n))
		return(list("Confidence Interval" = CI,
		"ATE Estimator" = tau.hat,
		"Estimated Upper Bound SD" = sigma.hat))
		
	}
	
	######### Load and prepare data ######### 
	
	# read data
	dat <- as.data.frame(haven::read_dta("Replication Data/3_mainexp_responses.dta"))
	
	# delete observation 167 due to encoding problem (Invalid byte sequence)
	dat <- dat[-167, ]
	
	
	# data transformations
	dat <- within(dat, {
		# Create class factor
		class <- factor(dplyr::case_when(
		male == 0 & maritalstatus == 0 ~ "Female & Single",
		male == 0 & maritalstatus != 0 ~ "Female & Non-single",
		male == 1 & maritalstatus == 0 ~ "Male & Single",
		male == 1 & maritalstatus != 0 ~ "Male & Non-single"
		))
		
		# treatment as factor
		treatment <- factor(ifelse(treatment == "A", "Private", "Public"))
		
		# add numerical treatment
		treatnum <- as.numeric(treatment == "Public")
		
		# create continous compensation and hours worked variables
		compensation <- sapply(regmatches(desiredcompensation, 
		gregexpr("?[0-9]+[.]?[0-9]*",
		desiredcompensation)),
		function(x) mean(as.numeric(x)))
		hours <- sapply(regmatches(hourswork,
		gregexpr("?[0-9]+[.]?[0-9]*", hourswork)),
		function(x) mean(as.numeric(x)))
		
		
		
	})
	
	
	
	######### Tables & Plots #########
	
	# summary table
	vtable::sumtable(dat[, c("treatment", "male", "single",
	"class", "compensation", "hours")],
	out = "latex")
	
	
	# compute compensation and hours by class and estimate the ATE
	Results <- do.call(cbind, lapply(dat[, c("compensation", "hours")],
	function(x){
		mat <- tapply(x, list(dat$class, dat$treatment), mean, na.rm = TRUE)
		mat <- cbind(mat, "Difference" = mat[, "Public"]- mat[, "Private"])
		mat
	}))
	
	# export as tex table
	knitr::kable(Results, format = "latex", digits = 2)
	
	
	
	
	######### Bootstrap #########
	
	
	# population sizes
	p.single <- mean(dat$maritalstatus == 0, na.rm = TRUE)
	pop.sizes <- c(c(1-p.single, p.single) * 21000000,
	c(1-p.single, p.single) * 21500000)
	
	# bootstrap CIs for compensation and hours worked
	set.seed(1) # set seed such that results are reproducible
	
	# loop over classes
	Boot.CIs <- t(mapply(function(class, pop.size){
		# select class
		data <- dat[dat$class == class, ]
		
		sapply(c("compensation", "hours"), function(out){
			# compute CIs
			boot <- causal_boot(data, dep.var = out,
			treatment = "treatnum", N = pop.size)
			
			# combine table with CIs and point estimate
			res <- as.numeric(c("Lower Bound" = boot[["Confidence Interval"]]["Lower"],
			"Upper Bound" = boot[["Confidence Interval"]]["Upper"]))
			return(res)
		})
		
	}, levels(dat$class), pop.sizes))
	
	colnames(Boot.CIs) <- rep(c("Lower Bound", "Upper Bound"), 2)
	
	# export as tex table
	knitr::kable(Boot.CIs, format = "latex", digits = 2)
	
	
\end{lstlisting}


%====== References ========
\newpage
\bibliographystyle{imsart-nameyear}
\bibliography{lit}{}


\end{document}
